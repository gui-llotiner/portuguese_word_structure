{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Data\n",
    "\n",
    "Collects data from wikipedia pages, saves them and extract unique words and their's frequencies to uses on word structure analysis. It also checks if the language corpora it's already collected to skip calling the API multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libs\n",
    "import wikipedia\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:03:41.254522Z",
     "start_time": "2023-08-11T01:03:41.225557Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up directory and file paths\n",
    "corpus_dir = Path(\"language_corpora\")\n",
    "corpus_dir.mkdir(exist_ok=True)\n",
    "corpus_file = corpus_dir / \"portuguese_corpus.txt\"\n",
    "\n",
    "# Set Wikipedia language to Portuguese\n",
    "wikipedia.set_lang(\"pt\")\n",
    "\n",
    "# List of common Portuguese Wikipedia page topics\n",
    "search_terms = [\"Brasil\", \"Econômia\", \"Futebol\", \"História\", \"Cultura\"]\n",
    "\n",
    "# Collect text from Wikipedia pages\n",
    "def collect_wikipedia_text(search_terms, results_limit=2):\n",
    "    collected_texts = []\n",
    "    total_words = 0\n",
    "    min_content_length = 500\n",
    "    target_words = 20000\n",
    "\n",
    "    for term in search_terms:\n",
    "        if total_words >= target_words:\n",
    "            break\n",
    "        try:\n",
    "            search_results = wikipedia.search(term, results=results_limit)\n",
    "            for title in search_results:\n",
    "                if total_words >= target_words:\n",
    "                    break\n",
    "                try:\n",
    "                    page = wikipedia.page(title, auto_suggest=False)\n",
    "                    content = page.content\n",
    "                    word_count = len(content.split())\n",
    "                    if word_count > min_content_length:\n",
    "                        collected_texts.append(content)\n",
    "                        total_words += word_count\n",
    "                    time.sleep(1)\n",
    "                except (wikipedia.exceptions.PageError, wikipedia.exceptions.DisambiguationError):\n",
    "                    pass\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return '\\n'.join(collected_texts)\n",
    "\n",
    "# Check if corpus exists and has sufficient words\n",
    "min_words = 20000\n",
    "if corpus_file.exists():\n",
    "    with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    word_count = len(text.split())\n",
    "    if word_count >= min_words:\n",
    "        pass  # Use existing corpus\n",
    "    else:\n",
    "        text = collect_wikipedia_text(search_terms)\n",
    "        if text and len(text.split()) >= min_words:\n",
    "            with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "else:\n",
    "    text = collect_wikipedia_text(search_terms)\n",
    "    if text and len(text.split()) >= min_words:\n",
    "        with open(corpus_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "# Check if text was retrieved or loaded\n",
    "if not text:\n",
    "    exit()\n",
    "\n",
    "# Clean the text: keep letters and diacritics, remove numbers, punctuation, spaces, etc.\n",
    "words = re.findall(r'[a-záéíóúâêîôûãõç]+', text.lower())\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# Create DataFrame with words and their frequencies\n",
    "df = pd.DataFrame(list(word_counts.items()), columns=['Palavra', 'Frequencia'])\n",
    "\n",
    "# Sort by frequency and sample 10 random words\n",
    "df = df.sort_values(by='Frequencia', ascending=False).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encontrando Sílabas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:03:41.270437Z",
     "start_time": "2023-08-11T01:03:41.258469Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cria uma lista de vogais\n",
    "vogais = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "# Cria uma lista de consoantes que aceitam CA\n",
    "consoantes_ca = ['b', 'c', 'd', 'f', 'g', 'p', 't']\n",
    "\n",
    "# Cria uma lista de consoantes auxiliares (CA)\n",
    "consoantes_auxiliares = ['l', 'r']\n",
    "\n",
    "# Cria uma lista de consoantes de coda (CC)\n",
    "consoantes_coda = ['m', 'n', 'r', 'l', 'z', 's', 'x']\n",
    "codas = consoantes_coda\n",
    "\n",
    "# Cria uma lista de consoantes que não aceitam CA\n",
    "consoantes_nao_ca = [\n",
    "    'h', 'j', 'm', 'n', 'r','l', 's', 'v', 'x', 'z', 'lh', 'nh', 'qu', 'gu', 'cl',\n",
    "    'ch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:03:41.300565Z",
     "start_time": "2023-08-11T01:03:41.272432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>silabas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>chur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>chul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>chuz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>chus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>chux</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1170 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     silabas\n",
       "0          a\n",
       "1         am\n",
       "2         an\n",
       "3        ans\n",
       "4         ar\n",
       "...      ...\n",
       "1165    chur\n",
       "1166    chul\n",
       "1167    chuz\n",
       "1168    chus\n",
       "1169    chux\n",
       "\n",
       "[1170 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gerar_silabas(vogais, consoantes_ca, consoantes_auxiliares, consoantes_coda, consoantes_nao_ca):\n",
    "    silabas = []\n",
    "    \n",
    "    # Using only vogais\n",
    "    for vogal in vogais:\n",
    "        silabas.append(vogal)\n",
    "        for cc in consoantes_coda:\n",
    "            silabas.append(vogal + cc)\n",
    "            if cc == 'n':\n",
    "                silabas.append(vogal + cc + 's')\n",
    "                \n",
    "    # Using consoantes_ca\n",
    "    for consoante in consoantes_ca:\n",
    "        for vogal in vogais:\n",
    "            silabas.append(consoante + vogal)\n",
    "            for cc in consoantes_coda:\n",
    "                silabas.append(consoante + vogal + cc)\n",
    "                if cc == 'n':\n",
    "                    silabas.append(consoante + vogal + cc + 's')\n",
    "                    \n",
    "    # Using consoantes_auxiliares\n",
    "    for consoante in consoantes_auxiliares:\n",
    "        for vogal in vogais:\n",
    "            silabas.append(consoante + vogal)\n",
    "            for cc in consoantes_coda:\n",
    "                silabas.append(consoante + vogal + cc)\n",
    "                if cc == 'n':\n",
    "                    silabas.append(consoante + vogal + cc + 's')\n",
    "                    \n",
    "    # Using consoantes_nao_ca\n",
    "    for consoante in consoantes_nao_ca:\n",
    "        for vogal in vogais:\n",
    "            silabas.append(consoante + vogal)\n",
    "            for cc in consoantes_coda:\n",
    "                silabas.append(consoante + vogal + cc)\n",
    "                if cc == 'n':\n",
    "                    silabas.append(consoante + vogal + cc + 's')\n",
    "    \n",
    "    return silabas\n",
    "\n",
    "# Chama a função com os parâmetros especificados\n",
    "resultado = gerar_silabas(vogais, consoantes_ca, consoantes_auxiliares, consoantes_coda, consoantes_nao_ca)\n",
    "\n",
    "# Converte em um dataframe\n",
    "silabas = pd.DataFrame(resultado, columns=['silabas'])\n",
    "\n",
    "silabas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:03:41.331593Z",
     "start_time": "2023-08-11T01:03:41.318791Z"
    }
   },
   "outputs": [],
   "source": [
    "def match_syllable_from_start(word, patterns, next_char_cond=None):\n",
    "    \"\"\"Return the longest syllable from the start of the word that matches a pattern.\"\"\"\n",
    "    max_syllable = ''\n",
    "    for pattern in patterns:\n",
    "        if word.startswith(pattern) and len(pattern) > len(max_syllable):\n",
    "            # If there's a condition to check the character after the matched pattern\n",
    "            next_char_index = len(pattern)\n",
    "            if next_char_cond:\n",
    "                # Either the word ends after the coda, or the next character satisfies the condition\n",
    "                if next_char_index == len(word) or (next_char_index < len(word) and next_char_cond(word[next_char_index])):\n",
    "                    max_syllable = pattern\n",
    "            else:\n",
    "                max_syllable = pattern\n",
    "    return max_syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:03:41.363531Z",
     "start_time": "2023-08-11T01:03:41.333578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palavra</th>\n",
       "      <th>Frequencia</th>\n",
       "      <th>Identified_Syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>típicos</td>\n",
       "      <td>1</td>\n",
       "      <td>[pi, cos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>jabor</td>\n",
       "      <td>1</td>\n",
       "      <td>[ja, bor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>arbitrariamente</td>\n",
       "      <td>1</td>\n",
       "      <td>[ar, bi, tra, ri, a, men, te]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>almeida</td>\n",
       "      <td>1</td>\n",
       "      <td>[al, me, i, da]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3858</th>\n",
       "      <td>tratar</td>\n",
       "      <td>2</td>\n",
       "      <td>[tra, tar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>desmembrado</td>\n",
       "      <td>1</td>\n",
       "      <td>[des, mem, bra, do]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>eleita</td>\n",
       "      <td>1</td>\n",
       "      <td>[e, le, i, ta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>aprovaram</td>\n",
       "      <td>1</td>\n",
       "      <td>[a, pro, va, ram]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>carcerária</td>\n",
       "      <td>1</td>\n",
       "      <td>[car, cer, ri, a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>lisboa</td>\n",
       "      <td>4</td>\n",
       "      <td>[lis, bo, a]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Palavra  Frequencia           Identified_Syllables\n",
       "3173          típicos           1                      [pi, cos]\n",
       "2902            jabor           1                      [ja, bor]\n",
       "5375  arbitrariamente           1  [ar, bi, tra, ri, a, men, te]\n",
       "3005          almeida           1                [al, me, i, da]\n",
       "3858           tratar           2                     [tra, tar]\n",
       "3569      desmembrado           1            [des, mem, bra, do]\n",
       "1311           eleita           1                 [e, le, i, ta]\n",
       "1278        aprovaram           1              [a, pro, va, ram]\n",
       "2129       carcerária           1              [car, cer, ri, a]\n",
       "212            lisboa           4                   [lis, bo, a]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identify_syllables_in_word(word, vogais, consoantes_ca, consoantes_auxiliares, consoantes_coda, consoantes_nao_ca):\n",
    "    identified_syllables = []\n",
    "    i = 0\n",
    "\n",
    "    # Patterns list comprehension\n",
    "    ca_vc_coda = [con + ca + v + co for con in consoantes_ca for ca in consoantes_auxiliares for v in vogais for co in consoantes_coda]\n",
    "    c_v_coda = [con + v + co for con in (consoantes_ca + consoantes_nao_ca) for v in vogais for co in consoantes_coda]\n",
    "    ca_v = [con + ca + v for con in consoantes_ca for ca in consoantes_auxiliares for v in vogais]\n",
    "    c_v = [con + v for con in (consoantes_ca + consoantes_nao_ca) for v in vogais]\n",
    "    v_coda = [v + co for v in vogais for co in consoantes_coda]\n",
    "    \n",
    "    while i < len(word):\n",
    "        patterns = [ca_vc_coda, c_v_coda, ca_v, c_v, v_coda, vogais]\n",
    "        \n",
    "        matched = False\n",
    "        for pattern_list in patterns:\n",
    "            syllable = match_syllable_from_start(word[i:], pattern_list)\n",
    "            \n",
    "            # For codas, check if it should be retained as a coda or act as the starting consonant of the next syllable\n",
    "            if syllable and syllable[-1] in consoantes_coda:\n",
    "                if i + len(syllable) < len(word) and word[i + len(syllable)] in vogais:\n",
    "                    # If coda is followed by a vowel, then treat it as the beginning of the next syllable\n",
    "                    syllable = syllable[:-1]\n",
    "            \n",
    "            if syllable:\n",
    "                identified_syllables.append(syllable)\n",
    "                i += len(syllable)\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            i += 1  # If no syllable match, just move to the next character\n",
    "\n",
    "    return identified_syllables\n",
    "\n",
    "df['Identified_Syllables'] = df['Palavra'].apply(lambda word: identify_syllables_in_word(word, vogais, consoantes_ca, consoantes_auxiliares, consoantes_coda, consoantes_nao_ca))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-11T01:03:41.379433Z",
     "start_time": "2023-08-11T01:03:41.365492Z"
    }
   },
   "outputs": [],
   "source": [
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#word = str(input('Digite a palavra: ')).lower()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m word0 = \u001b[43mword\u001b[49m.removesuffix(\u001b[33m'\u001b[39m\u001b[33ms\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m word1 = word0[:-\u001b[32m2\u001b[39m]\n\u001b[32m      4\u001b[39m word2 = word0[:-\u001b[32m3\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'word' is not defined"
     ]
    }
   ],
   "source": [
    "#word = str(input('Digite a palavra: ')).lower()\n",
    "word0 = word.removesuffix('s')\n",
    "word1 = word0[:-2]\n",
    "word2 = word0[:-3]\n",
    "\n",
    "# Listas de Terminações\n",
    "vogox = ['á', 'é', 'ê', 'i', 'í', 'ó', 'ô', 'u', 'ú', 'ã',\n",
    "         'ão', 'õe', 'ãe', 'ém']\n",
    "conox = ['r', 'l', 'z', 'x', 'om', 'im', 'um']\n",
    "vogpro = ['á', 'â', 'é', 'ê', 'í', 'ó', 'ô', 'ú']\n",
    "vogsim = ['a', 'e', 'o']\n",
    "excecao = ['ã']\n",
    "\n",
    "# Grupos de Palavras\n",
    "\n",
    "grupo_a = word0.endswith(tuple(vogox)) #Termina em elementos oxitonos\n",
    "grupo_b = word0.endswith(tuple(conox)) #Termina em consoante\n",
    "grupo_c = bool(set(vogpro) & set(word0)) #Contém vogal acentuada\n",
    "grupo_d = word0.endswith(tuple(vogsim)) #Termina em vogal não-acentuada\n",
    "grupo_e = bool(set(vogpro) & set(word1)) #Contém vogal acentuada (não terminal -2)\n",
    "grupo_f = bool(set(excecao) & set(word0)) #Contém ã\n",
    "grupo_g = bool(set(vogpro) & set(word2)) #Contém vogal acentuada (não terminal -3)\n",
    "\n",
    "# Respostas - Vogais\n",
    "if (grupo_c and grupo_d and grupo_g == True) and grupo_f == False:\n",
    "    print('Essa palavra é proparoxítona')\n",
    "elif grupo_a == True and grupo_e == False:\n",
    "    print('Essa palavra é oxítona')\n",
    "\n",
    "# Respostas - Consoantes\n",
    "\n",
    "elif grupo_b == True and grupo_c == False:\n",
    "    print('Essa palavra é oxítona')\n",
    "\n",
    "elif grupo_b and grupo_c == True:\n",
    "    print('Essa palavra é paroxítona')\n",
    "\n",
    "else:\n",
    "    print('Essa palavra é paroxítona')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
